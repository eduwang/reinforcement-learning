# 🤖 강화학습 실험실 (Reinforcement Learning Lab)

웹 브라우저에서 실행되는 인터랙티브 강화학습 실험 환경입니다. 다양한 강화학습 알고리즘을 시각적으로 학습하고 실험할 수 있습니다.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![JavaScript](https://img.shields.io/badge/JavaScript-ES6+-yellow.svg)
![Vite](https://img.shields.io/badge/Vite-7.1.7-646CFF.svg)

## 📋 목차

- [프로젝트 소개](#프로젝트-소개)
- [주요 기능](#주요-기능)
- [기술 스택](#기술-스택)
- [설치 및 실행](#설치-및-실행)
- [프로젝트 구조](#프로젝트-구조)
- [사용 방법](#사용-방법)
- [알고리즘 설명](#알고리즘-설명)
- [하이퍼파라미터 가이드](#하이퍼파라미터-가이드)

## 🎯 프로젝트 소개

이 프로젝트는 강화학습의 핵심 개념을 학습하고 실험할 수 있는 웹 기반 인터랙티브 플랫폼입니다. 세 가지 고전적인 강화학습 문제를 제공하며, 실시간으로 학습 과정을 시각화합니다.

### 제공되는 환경

1. **🎯 CartPole (DQN)**
   - 막대기를 카트 위에 균형있게 세우는 제어 문제
   - Deep Q-Network (DQN) 알고리즘 사용
   - 연속적인 균형 유지가 목표

2. **🏃 미로 찾기 (Q-Learning)**
   - 에이전트가 미로를 탐험하여 최단 경로를 찾는 문제
   - Q-Learning (테이블 기반) 알고리즘 사용
   - 최적 경로 학습이 목표

3. **🚀 Lunar Lander (DQN)**
   - 우주선을 착륙 패드에 안전하게 착륙시키는 문제
   - Deep Q-Network (DQN) 알고리즘 사용
   - 적절한 속도와 각도로 착륙하는 것이 목표

## ✨ 주요 기능

- **실시간 시각화**: 학습 과정을 실시간으로 확인
- **인터랙티브 제어**: 하이퍼파라미터를 슬라이더로 조정
- **학습 속도 조절**: x1, x3, x5, x10, MAX 속도 지원
- **성능 차트**: 에피소드별 점수/스텝 수 그래프
- **모델 저장/로드**: 학습된 모델을 저장하고 불러오기 (CartPole)
- **상세한 통계**: 에피소드, 점수, 탐험률 등 실시간 정보
- **반응형 UI**: 직관적이고 현대적인 사용자 인터페이스

## 🛠 기술 스택

- **프론트엔드**: Vanilla JavaScript (ES6+)
- **빌드 도구**: Vite 7.1.7
- **차트 라이브러리**: Chart.js 4.5.0 (참고용, 현재는 Canvas 직접 구현)
- **스타일링**: CSS3 (그라데이션, 애니메이션)

## 📦 설치 및 실행

### 사전 요구사항

- Node.js 16.0 이상
- npm 또는 yarn

### 설치

```bash
# 저장소 클론
git clone <repository-url>
cd reinforcement-learning

# 의존성 설치
npm install
```

### 실행

```bash
# 개발 서버 시작
npm run dev

# 브라우저에서 http://localhost:5173 접속
```

### 빌드

```bash
# 프로덕션 빌드
npm run build

# 빌드 결과 미리보기
npm run preview
```

## 📁 프로젝트 구조

```
reinforcement-learning/
├── index.html              # HTML 진입점
├── package.json            # 프로젝트 설정 및 의존성
├── vite.config.js          # Vite 설정 (있는 경우)
├── public/                 # 정적 파일
│   └── vite.svg
└── src/
    ├── main.js             # 메인 애플리케이션 로직
    ├── style.css           # 스타일시트
    ├── cartpole.js         # CartPole 환경 구현
    ├── lunarlander.js      # Lunar Lander 환경 구현
    ├── maze.js             # 미로 찾기 환경 구현
    ├── dqn.js              # DQN 알고리즘 구현
    ├── qlearning.js        # Q-Learning 알고리즘 구현
    └── counter.js          # 유틸리티 (있는 경우)
```

## 🎮 사용 방법

### CartPole

1. **CartPole** 탭 선택
2. 하이퍼파라미터 조정:
   - 학습률 (Learning Rate): 0.0001 ~ 0.01
   - 탐험률 (Epsilon): 0.01 ~ 1.0
   - 탐험 감소율: 0.99 ~ 0.999
   - 할인율 (Gamma): 0.8 ~ 0.99
   - 은닉층 크기: 64, 128, 256, 512
3. 학습 속도 선택 (x1, x3, x5, x10, MAX)
4. **학습 시작** 버튼 클릭
5. 실시간으로 학습 과정 관찰
6. 필요시 **모델 저장** 또는 **모델 불러오기**

### 미로 찾기

1. **미로 찾기** 탭 선택
2. 하이퍼파라미터 조정:
   - 학습률: 0.01 ~ 0.9
   - 탐험률: 0.01 ~ 1.0
   - 탐험 감소율: 0.99 ~ 0.999
   - 할인율: 0.8 ~ 0.99
   - 미로 크기: 5x5, 8x8, 10x10
3. 학습 속도 선택
4. **학습 시작** 버튼 클릭
5. 에이전트의 경로 탐색 과정 관찰
6. **새 미로 생성**으로 다른 미로 시도

### Lunar Lander

1. **Lunar Lander** 탭 선택
2. 하이퍼파라미터 조정:
   - 학습률: 0.0001 ~ 0.01
   - 탐험률: 0.01 ~ 1.0
   - 할인율: 0.8 ~ 0.99
   - 은닉층 크기: 64, 128, 256
3. 학습 속도 선택
4. **학습 시작** 버튼 클릭
5. 우주선의 착륙 시도 과정 관찰

## 🧠 알고리즘 설명

### Deep Q-Network (DQN)

**사용 환경**: CartPole, Lunar Lander

DQN은 신경망을 사용하여 Q값을 근사하는 알고리즘입니다.

**주요 특징**:
- **경험 리플레이 (Experience Replay)**: 과거 경험을 저장하고 랜덤 샘플링하여 학습
- **타겟 네트워크 (Target Network)**: 안정적인 학습을 위한 별도의 타겟 네트워크
- **Epsilon-Greedy 탐험**: 탐험과 활용의 균형

**구현 세부사항**:
- 2층 신경망 (입력층 → 은닉층 → 출력층)
- ReLU 활성화 함수
- 그래디언트 클리핑으로 학습 안정화
- 보상 스케일링으로 수렴 개선

### Q-Learning

**사용 환경**: 미로 찾기

Q-Learning은 테이블 기반 강화학습 알고리즘입니다.

**주요 특징**:
- **Q-Table**: 상태-행동 쌍의 Q값을 저장하는 테이블
- **탐험 vs 활용**: Epsilon-greedy 정책으로 균형 유지
- **시간차 학습**: 즉시 보상과 미래 가치를 결합

**구현 세부사항**:
- Map 기반 Q-Table (동적 확장)
- 상태를 문자열 키로 변환하여 저장
- 최적 행동 선택 및 Q값 업데이트

## ⚙️ 하이퍼파라미터 가이드

### 학습률 (Learning Rate)

- **높은 값 (0.01)**: 빠른 학습, 불안정할 수 있음
- **낮은 값 (0.0001)**: 느린 학습, 안정적
- **권장값**: 
  - CartPole: 0.001
  - Lunar Lander: 0.0005
  - 미로 찾기: 0.1

### 탐험률 (Epsilon)

- **높은 값 (0.99)**: 많은 탐험, 새로운 전략 시도
- **낮은 값 (0.1)**: 많은 활용, 학습된 전략 사용
- 초기에는 높게 시작하여 점차 감소

### 탐험 감소율 (Epsilon Decay)

- **높은 값 (0.999)**: 천천히 감소, 오래 탐험
- **낮은 값 (0.99)**: 빠르게 감소, 빨리 수렴
- **권장값**: 0.995 ~ 0.998

### 할인율 (Gamma)

- **높은 값 (0.99)**: 장기적 보상 중시
- **낮은 값 (0.8)**: 즉각적 보상 중시
- **권장값**: 0.9 ~ 0.99

### 은닉층 크기

- **작은 크기 (64)**: 빠른 학습, 단순한 패턴
- **큰 크기 (512)**: 복잡한 패턴 학습 가능, 느린 학습
- **권장값**: 128 (균형잡힌 선택)

## 📊 학습 진행 상황 해석

### CartPole
- **0-50 에피소드**: 경험 수집 단계 (짧은 점수)
- **50-150 에피소드**: 점진적 개선
- **150+ 에피소드**: 안정적인 고득점 (200+ 점수)

### 미로 찾기
- **0-50 에피소드**: 랜덤 탐험 (많은 스텝)
- **50-200 에피소드**: 경로 학습 중
- **200+ 에피소드**: 최적 경로 수렴 (최단 경로에 근접)

### Lunar Lander
- **0-100 에피소드**: 대부분 추락 (음수 점수)
- **100-300 에피소드**: 착륙 시도 시작
- **300-500 에피소드**: 가끔 성공
- **500+ 에피소드**: 안정적인 착륙 (100점+)

## 🎨 시각화 기능

- **실시간 렌더링**: 각 환경의 상태를 실시간으로 표시
- **학습 차트**: 에피소드별 성능 그래프
- **통계 정보**: 에피소드, 점수, 탐험률, 메모리 크기 등
- **히트맵**: 미로 찾기에서 방문한 셀 표시
- **경로 추적**: 에이전트의 이동 경로 시각화

## 🔧 문제 해결

### 학습이 수렴하지 않는 경우

1. 학습률을 낮춰보세요
2. 탐험률 감소율을 높여보세요 (더 천천히 감소)
3. 은닉층 크기를 조정해보세요
4. 더 많은 에피소드를 학습해보세요

### 성능이 좋지 않은 경우

1. 하이퍼파라미터를 기본값으로 재설정
2. 에이전트를 초기화하고 다시 학습
3. 학습 속도를 낮춰서 더 자세히 관찰

## 📝 라이선스

이 프로젝트는 MIT 라이선스를 따릅니다.

## 👤 작성자

**Hyowon Wang**

---

**참고**: 이 프로젝트는 교육 목적으로 제작되었으며, 강화학습의 기본 개념을 이해하고 실험하기 위한 도구입니다. 실제 프로덕션 환경에서 사용하기 위해서는 추가적인 최적화가 필요할 수 있습니다.

